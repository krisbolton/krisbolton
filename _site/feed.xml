<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kris Bolton</title>
    <description>The personal website of Kris Bolton, Cyber Security Research Student.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 30 Apr 2020 10:09:17 +0100</pubDate>
    <lastBuildDate>Thu, 30 Apr 2020 10:09:17 +0100</lastBuildDate>
    <generator>Jekyll v3.6.3</generator>
    
      <item>
        <title>Experimenting with the Modern Honeypot Network (MHN)</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://github.com/pwnlandia/mhn&quot;&gt;MHN&lt;/a&gt; is an open-source centralized server for managing honeypots, making deployment and data collection simple and fast. MHN currently supports a number of honeypot types, check out the &lt;a href=&quot;https://github.com/pwnlandia/mhn/wiki/List-of-Supported-Sensors&quot;&gt;list of supported sensors&lt;/a&gt; for details, they also allow &lt;a href=&quot;https://github.com/pwnlandia/mhn/wiki/Add-Support-for-New-Sensors-to-the-MHN&quot;&gt;adding new sensors&lt;/a&gt; should you wish.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;
&lt;p&gt;My goal was to collect malware and aggregate statistics on various connections and attacks against a single sensor server I deployed. The architecture was two nano linode ubuntu 16.04 instances, one acting as the server and the other as the sensor. This turned out well, the nano instance was enough to handle the workload the majority of the time.&lt;/p&gt;

&lt;h4 id=&quot;server-setup&quot;&gt;Server setup&lt;/h4&gt;

&lt;p&gt;Find a server provider, I chose &lt;a href=&quot;https://www.linode.com/&quot;&gt;Linode&lt;/a&gt;. I went with the nano configuration; 1GB RAM, 1CPU, 25GB storage, 1TB bandwidth for $5 per month. Enough for a small setup, as you add sensors the server will need more resources, so provision accordingly.&lt;/p&gt;

&lt;p&gt;Give your server a descriptive name, e.g. “mhn-server”. Ubuntu 16.04 is recommended by MHN as its been tested and verified to work with MHN.&lt;/p&gt;

&lt;p&gt;Secure the mhn-server as your normally would, its intended for you to manage sensors, you’re not looking for attackers to attack this instance.&lt;/p&gt;

&lt;h4 id=&quot;server-steps&quot;&gt;Server steps&lt;/h4&gt;

&lt;h6 id=&quot;create-a-non-root-user&quot;&gt;Create a non-root user.&lt;/h6&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;adduser deploy&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Don’t install MHN as root, it will not work as intended.&lt;/p&gt;

&lt;h6 id=&quot;add-deploy-as-sudo-user-under-user-privilege-specification&quot;&gt;Add deploy as sudo user under ‘user privilege specification’:&lt;/h6&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;visudo&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;deploy ALL=(ALL:ALL) ALL&lt;/code&gt;&lt;/p&gt;

&lt;h6 id=&quot;switch-to-the-deploy-user&quot;&gt;Switch to the deploy user:&lt;/h6&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;su deploy&lt;/code&gt;&lt;/p&gt;

&lt;h6 id=&quot;clone-the-mhn-github-repo&quot;&gt;Clone the MHN GitHub repo:&lt;/h6&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git clone https://github.com/pwnlandia/mhn.git&lt;/code&gt;&lt;/p&gt;

&lt;h6 id=&quot;change-to-the-mhn-directory&quot;&gt;Change to the mhn directory:&lt;/h6&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd mhn&lt;/code&gt;&lt;/p&gt;

&lt;h6 id=&quot;run-the-installation-script&quot;&gt;Run the installation script:&lt;/h6&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ./install.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Towards the end of the installation you’ll be asked a series of questions. I chose no debug mode, I added a new gmail I created - &lt;strong&gt;this will also be used to login to the web interface&lt;/strong&gt;. I didn’t need splunk or ELK. I left the rest of the options as default.&lt;/p&gt;

&lt;h6 id=&quot;login-to-the-web-interface&quot;&gt;Login to the web interface.&lt;/h6&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ipconfig&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Copy &amp;amp; paste the IP into a browser on your machine, and login with the email and password you provided earlier.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The server’s done!&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;sensor-setup&quot;&gt;Sensor setup&lt;/h4&gt;

&lt;p&gt;Create a second instance on your service provider, again using a descriptive name - this helps if you scale things up later. I went with ‘mhn-snort-dionaea’.&lt;/p&gt;

&lt;p&gt;Snort is an intrusion detection system (IDS), which will provide detail on attacks against the server. It can be configured extensively, see the &lt;a href=&quot;https://www.snort.org/documents&quot;&gt;snort documentation&lt;/a&gt; for more.&lt;/p&gt;

&lt;p&gt;Dionaea is a a honeypot emulating various network services to capture malware. See the &lt;a href=&quot;https://dionaea.readthedocs.io/en/latest/&quot;&gt;dionaea documentation&lt;/a&gt; for specific configuration details.&lt;/p&gt;

&lt;h4 id=&quot;sensor-steps&quot;&gt;Sensor steps&lt;/h4&gt;

&lt;h6 id=&quot;select-a-script&quot;&gt;Select a script&lt;/h6&gt;

&lt;p&gt;In the web interface provided by the mhn-server, go to the deploy tab and chose the sensor you want to deploy from the ‘select script’ drop down menu.&lt;/p&gt;

&lt;h6 id=&quot;copy--paste-wget-string&quot;&gt;Copy &amp;amp; paste wget string&lt;/h6&gt;

&lt;p&gt;Sensor setup is as easy as copy &amp;amp; pasting the &lt;code class=&quot;highlighter-rouge&quot;&gt;wget&lt;/code&gt; command string into the sensor instance.&lt;/p&gt;

&lt;h6 id=&quot;check-the-sensors-tab&quot;&gt;Check the sensors tab&lt;/h6&gt;

&lt;p&gt;Click the sensors tab in the web interface to view your new sensor.&lt;/p&gt;

&lt;h6 id=&quot;dionaea&quot;&gt;Dionaea&lt;/h6&gt;

&lt;p&gt;Follow the same steps copy &amp;amp; pasting the &lt;code class=&quot;highlighter-rouge&quot;&gt;wget&lt;/code&gt; script string for dionaea into the mhn-snort-dionaea instance.&lt;/p&gt;

&lt;p&gt;Now you have snort and dionaea setup and running. You can view incoming attacks in the web interface.&lt;/p&gt;

&lt;h3 id=&quot;collecting-data&quot;&gt;Collecting data&lt;/h3&gt;

&lt;p&gt;As mentioned towards the beginning, my intention was to gather aggregate statistics and hopefully some malware files. Your goals may be different and you can add different sensors and service configurations to achieve your goals.&lt;/p&gt;

&lt;p&gt;Immediately, I saw a small number of attacks coming in, surprising how quickly your services are found.&lt;/p&gt;

&lt;p&gt;Within 24 hours I had logged over &lt;em&gt;300,000&lt;/em&gt; scans and attacks from across the world. The web interface provides an enticing global attack map (below). The yellow dot shows the location of the server and the red dots show attacker locations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2020/honeypot-map.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The sensor CPU handled the incoming traffic well, utilising around 40% max (observed). As scans/attacks increase between approx. 08:00 and 11:50 I turn on &lt;code class=&quot;highlighter-rouge&quot;&gt;ufw&lt;/code&gt;, the linux firewall, to observe the effect out of interest. This is why the graph shows connections and CPU suddenly drop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2020/cpuhoneypot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2020/traffichoneypot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Later I turned the firewall off and resumed data collection.&lt;/p&gt;

&lt;p&gt;I’m planning on putting together a more detailed post of the data I captured. It won’t be anything particularly special, mostly data on which ports were most attacked, perhaps hints at attack behaviour. Dionaea did capture some files, but I’ve yet to take a look.&lt;/p&gt;

&lt;p&gt;Look out for the details and lessons learned in an up-coming post.&lt;/p&gt;

&lt;h4 id=&quot;reminder&quot;&gt;Reminder&lt;/h4&gt;

&lt;p&gt;Whenever you do anything with malware consider the legal implications. Here you are collecting malware - likely a legal activity within your country. However, this quickly changes should you distribute it. Setting the server to be publicly available so you can download the malware, for example. To be clear, this is not legal advice, I am not a legal expert. As ever, seek advice specific to your location and proceed with caution.&lt;/p&gt;
</description>
        <pubDate>Wed, 22 Apr 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/experimenting-with-the-modern-honey-network</link>
        <guid isPermaLink="true">http://localhost:4000/experimenting-with-the-modern-honey-network</guid>
        
        <category>Security</category>
        
        <category>Honeypots</category>
        
        
      </item>
    
      <item>
        <title>UK Cyber Security Spending First Results: National Ambulance Services</title>
        <description>&lt;p&gt;In late 2018 I started sending Freedom of Information Requests (FOI) to various UK Government Departments, Agencies and Non-Governmental Bodies asking them to disclose their yearly spending data on cyber security.&lt;/p&gt;

&lt;p&gt;My assertion is cyber security spending is an indication of the overall security of an organiation and how seriously they take security.&lt;/p&gt;

&lt;p&gt;Initially I started with services which can be considered of naitonal importance, and the lack of security could be considered a risk to national security. This post will briefly dicuss the responses of UK national ambulance services from Northern Ireland, Scotland, Wales and England.&lt;/p&gt;

&lt;h3 id=&quot;the-foi-request&quot;&gt;The FOI Request&lt;/h3&gt;

&lt;p&gt;Each Department, Agency or Non-Governmental Body was asked to provide the same information:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide the total amount of money spent on Cyber Security for financial years 2015-18.&lt;/li&gt;
  &lt;li&gt;The term “Cyber Security” is defined as: “… consider the term to encompass activities relating to information security, computer security and computer network security. Including staff training, consultant services, software and hardware.”&lt;/li&gt;
  &lt;li&gt;Additionaly, “Details on what training and consultant services were for …”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the time of the request (December 2018) 1. would provide approximately three years of data.&lt;/p&gt;

&lt;p&gt;All of the information I collect and the exact request wording is available on &lt;a href=&quot;https://github.com/krisbolton/UK-Cyber-Security-Spending-Data&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;status-of-resquests&quot;&gt;Status of Resquests&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/uk-amb-table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-results-so-far&quot;&gt;The Results So Far&lt;/h3&gt;

&lt;p&gt;I haven’t finished recieving the data from some of the ambulance services, notably the West Midlands and South Central - they’re overdue. I’ll be following up soon. Additionally, due to the structure of some of the services, namely the three remote island services; Guernsey, Isle of Man and States of Jersey, I have found it difficult to contact the appropraite people. Eventually this will be complete.&lt;/p&gt;

&lt;p&gt;It must be pointed out I am not a statistician, and as different entities do not record or store financial data in the same way this make analysis difficult.&lt;/p&gt;

&lt;p&gt;OK! On to some results.&lt;/p&gt;

&lt;p&gt;It is clear there is a large disparity between the spending of some abulance services compared to others. Excluding outliers, and focusing on a particular category, Network Security for example, and the financial year 2016/7, East Midlands spent £1,275, North East Ambulance spent £4,175 and the Welsh service spent £46,947.&lt;/p&gt;

&lt;p&gt;East Midlands Ambulance Service spends significantly different amounts per year, although the default appears to be very low - they did not specificy details so it is difficult to analyse. In 2015 and 2018 they spend over £100,000, while 2016 and 2017 they spent a mere £1,275 each year.&lt;/p&gt;

&lt;p&gt;The North East Ambulance Service is consistent in its spending on Network Security over the year period that they provided data. In 2015 and 2016 they spent £4,175 and £4,000 in 2018. While in 2017 they spent £57,375, the increase was indicated as capital expenditure on improvements in perimeter security.&lt;/p&gt;

&lt;p&gt;The Welsh Ambulance Service is somewhat similar to East Midlands, although there is perhaps a trend towards higher spend after WannaCry(?). They spent £5,337 in 2015/6, and £46,793 and £54,727 in 2016/7 and 2017/8 respectively.&lt;/p&gt;

&lt;p&gt;It is difficult to analyse due to the lack of insight into how they are actually spending the money. However, it isn’t possible to obtain this data due to the security risk of providing detailed explanations of such spending. I purposefully suggested recipients do not include specifics for this reason.&lt;/p&gt;

&lt;p&gt;It’s going to be an interesting project once it is complete. I also intend to collect data from the other services; police and fire. As well as Government Departments.&lt;/p&gt;

&lt;p&gt;Keep an eye on the &lt;a href=&quot;https://github.com/krisbolton/UK-Cyber-Security-Spending-Data&quot;&gt;GitHub&lt;/a&gt; for regular updates, or check back here in a month or two.&lt;/p&gt;
</description>
        <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/uk-cyber-security-spending</link>
        <guid isPermaLink="true">http://localhost:4000/uk-cyber-security-spending</guid>
        
        <category>Projects</category>
        
        
      </item>
    
      <item>
        <title>A Practical Introduction to Artificial Neural Networks with Python</title>
        <description>&lt;h3 id=&quot;an-introduction-to-artificial-neural-networks&quot;&gt;An Introduction to Artificial Neural Networks&lt;/h3&gt;
&lt;p&gt;Artificial neurons mimic the basic function of biological neurons, and much like their biological counterparts they only become useful when connected in a larger network, called Artificial Neural Networks. Neural networks may mimic the basic building block of the human brain, however, when connected to a larger network artificial neurons do not inherit their biological counterparts ability of what is termed “general intelligence”. Even the largest collection of artificial neurons are not intelligent. Artificial Neural Networks are only capable of performing specific tasks.&lt;/p&gt;

&lt;!--more--&gt;

&lt;blockquote&gt;
  &lt;p&gt;neural networks can achieve above 80% accuracy in certain tasks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Neural networks are extremely adept at recognising patterns, this is pattern recognition is the reason they have become so widely used and talked about in recent years. The ability of neural networks to discover and recognise patterns allows them to far surpass traditional methods in computer science, making them viable for use in research and business.&lt;/p&gt;

&lt;p&gt;This pattern recognition ability can be used to solve a wide variety of problems. One of the projects in the latter half of this book uses sentiment analysis, a natural language processing categorisation problem. Is a string of text positive, neutral or negative? This question may seem simple at first, however, the algorithm will have to traverse the nuances in human speech; sarcasm and other complex linguistic nuances can blur the question for machines. Despite this, neural networks can achieve greater than 70% accuracy, and the most advanced neural networks can achieve above 80% in sentiment analysis and opinion mining tasks. This is more than enough to provide advantages over using humans to categorise sentiment.&lt;/p&gt;

&lt;h3 id=&quot;an-artificial-neuron&quot;&gt;An Artificial Neuron&lt;/h3&gt;
&lt;p&gt;The image below shows an illustration of a single biological neuron annotated to describe a single artificial neurons function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/neuron-annotated.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A biological neuron receives input signals from its dendrites from other neurons and sends output signals along its axon, which branches out and connects to other neurons. In the illustration above, the input signal is represented by x0, as this signal ‘travels’ it is multiplied (&lt;code class=&quot;highlighter-rouge&quot;&gt;w0 x0&lt;/code&gt;) based on the weight variable (&lt;code class=&quot;highlighter-rouge&quot;&gt;w0&lt;/code&gt;). The weight variables are learnable and the weights strength and polarity (positive or negative) control the influence of the signal. The influence is determined by summing the signal input and weight (&lt;code class=&quot;highlighter-rouge&quot;&gt;∑i wi xi + b&lt;/code&gt;) which is then calculated by the activation function f, if it is above a certain threshold the neuron fires. Below is a simplified diagram.&lt;/p&gt;

&lt;h3 id=&quot;sigma&quot;&gt;Sigma&lt;/h3&gt;
&lt;p&gt;The Greek letter sigma &lt;code class=&quot;highlighter-rouge&quot;&gt;∑&lt;/code&gt; is used to represent summation, this can be thought of as analogous to a  for loop in programming.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/summation.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;types-of-artificial-neuron&quot;&gt;Types of Artificial Neuron&lt;/h3&gt;
&lt;p&gt;We’ve hinted at the existence of different types of neurons which serve different purposes. In this section we will discuss the different types of neuron which will make up a neural network. Networks don’t have to have every type of neuron, and some neurons can have multiple purposes.&lt;/p&gt;

&lt;p&gt;Neuron naming conventions differ between sources, industries, and people of different backgrounds. Neurons might also be known as units or nodes. They’re simply different names for the same thing.&lt;/p&gt;

&lt;h4 id=&quot;input-and-output-neurons&quot;&gt;Input and Output Neurons&lt;/h4&gt;
&lt;p&gt;Input and output neurons can be thought of as placeholders which represent information passed into the network and information processed out from the network in the form of vectors or arrays. These vectors typically contain floating point numbers. The number of elements within the vector is equal to the number of input neurons.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/typesofneuron1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;hidden-neurons&quot;&gt;Hidden Neurons&lt;/h4&gt;
&lt;p&gt;Hidden neurons sit in the middle of a network, surrounded by other neurons – they receive input from input neurons or other hidden neurons, and they output to output neurons or hidden neurons. They are never connected to the data or produce output themselves. This is their defining characteristic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/typesofneuron2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;bias-neurons&quot;&gt;Bias Neurons&lt;/h4&gt;
&lt;p&gt;The addition of bias into a network helps the network learn by allowing the programmer to shift the activation function curve to the left or right, the fine tuning of this parameter can affect the success of learning. Below is an example of a network including bias neurons.&lt;/p&gt;

&lt;p&gt;The diagram shows a neural network designed to compute XOR arguments. Each layer, excluding the input layer, has a bias neuron attached. Later on in this chapter we will build and calculate the output of this network, and build an example in scikit-learn.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/biasneuronexample.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An example of how a programmer can shift the activation function laterally:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/lateralshiftbias.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;context-neurons&quot;&gt;Context Neurons&lt;/h4&gt;
&lt;p&gt;Context neurons exist in some specific types of neural networks and are not present in all network types. For example, Recurrent neural networks use context neurons as a form of memory to hold on to information from past calculations. They attempt to mimic context created by biological phenomena within human brains. An analogy helps make their purpose clearer; if you’re crossing the street and you hear a car horn you will likely stop and look towards the noise, looking for danger. However, if you were at a sports event and hear a horn from an over enthusiastic supporter you ignore it. You’ve learned loud noises are important when crossing the street, but are meaningless within other contexts.&lt;/p&gt;

&lt;p&gt;Below is an example of a recurrent neural network architecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/contextneuronrrn.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The network resembles a typical network, with input I1, two hidden neurons and output O1. Additionally, information is duplicated and sent to C1 and C2 which provide data from previous calculations.&lt;/p&gt;

&lt;h4 id=&quot;weight&quot;&gt;Weight&lt;/h4&gt;
&lt;p&gt;Weight variables allow the network or programmer to adjust the program to closer match the desired output. Weights are programmatically manipulated by the network during training iterations to bring the error rate down as the network learns. As seen in the section discussing bias, bias allows the manipulation of the activation function left or right on a graph. Weight allows the gradient of the activation function to be manipulated.&lt;/p&gt;

&lt;h3 id=&quot;activation-functions-in-brief&quot;&gt;Activation Functions in Brief&lt;/h3&gt;
&lt;p&gt;The frequency of the firing of an artificial neuron is determined by an activation function. It determines at what threshold the neuron will fire. This section describes the Sigmoid activation function to bring clarity to the operation of an artificial neuron. Activation functions will be discussed in detail further on in this section.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/sigmoid-activation-function.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph above shows the Sigmoid activation function, which will convert all input signals into positive values between 0 and 1. The use of other activation functions with wider values, or negative values change the frequency of the firing of a neuron and will suit different types of data and purposes.&lt;/p&gt;

&lt;h3 id=&quot;practical-exercise&quot;&gt;Practical Exercise&lt;/h3&gt;

&lt;h3 id=&quot;the-perceptron&quot;&gt;The Perceptron&lt;/h3&gt;
&lt;p&gt;The foundation of modern neural networks was created in 1956 by Frank Rosenblatt and sort to mimic the architecture of a basic biological neuron, it was called a perceptron. This architecture can be seen in the illustration at the beginning of this chapter.&lt;/p&gt;

&lt;p&gt;Classifying flowers
Scikit-learn includes the Iris flower dataset which is often used for algorithm testing purposes, and it has become somewhat of a “hello world” dataset of neural networks. The aim of our single layer model is to classify flowers from the Iris dataset into different categories.&lt;/p&gt;

&lt;p&gt;Installing Scikit-learn via Anaconda
I recommend installing Anaconda, a scientific python library which includes scikit-learn, all of its dependencies, matlibplot and more. We’ll be using jupyter notebook, formally IPython Notebook, bundled with Anaconda to run our code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.anaconda.com/download&quot;&gt;https://www.anaconda.com/download&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you’re having trouble please refer to the scikit-learn or anaconda documentation. Or if conflicts are possible with your existing installations, consider using a virtual machine and start from a clean slate.&lt;/p&gt;

&lt;h3 id=&quot;jupyter-notebook&quot;&gt;Jupyter Notebook&lt;/h3&gt;
&lt;p&gt;Launch jupyter notebook via the Anaconda Navigator application, which was installed with Anaconda.&lt;/p&gt;

&lt;p&gt;Create a new Python notebook in the jupyter web interface which has launched.&lt;/p&gt;

&lt;p&gt;Now we can write python and run it live in the web interface.&lt;/p&gt;

&lt;h3 id=&quot;importing-dependencies&quot;&gt;Importing Dependencies&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;the-iris-data-set&quot;&gt;The Iris Data Set&lt;/h3&gt;
&lt;p&gt;The Iris dataset consists of 150 samples of four features per sample, measuring the length and width of sepals and petals (in cm), with 50 samples for three species of Iris (Iris setosa, Iris virginica, and Iris versicolor). First featured in British statistician and biologist Ronald Fisher’s research article in 1936 the data set has become a popular test of predictive algorithms, including neural networks.&lt;/p&gt;

&lt;p&gt;The Iris dataset is small in comparison to the data sets which are needed for more complex algorithms, for example, in deep learning convolutional neural networks require tens of thousands of data points to learn effectively.&lt;/p&gt;

&lt;p&gt;Itallics indicate comments or text output by the console.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Load the Iris dataset
iris = datasets.load_iris()

#Assign the data to vertices
x = iris.data
y = iris.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s take a look at the data we’re going to manipulate.&lt;/p&gt;

&lt;p&gt;x indicies:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Print the first five features (the four measurements in a 2D array)
x[:5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-&quot;&gt;
array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
       [5. , 3.6, 1.4, 0.2]])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;y indicies:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Print all 150 indices of x axis (the class of each sample)
y[:150]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-&quot;&gt;
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Neural Networks require the data to be represented as floating point numbers. This allows the decimal points high accuracy to be used by the various calculations within the underlying implementation. Floating point numbers can simply represent data with finer granularity than the ten representations that integers allow. This can be appreciated by looking at the annotated neuron illustration at the start of this section.&lt;/p&gt;

&lt;p&gt;The 2D array held in x represents four columns: Sepal Length, Sepal Width, Petal Length and Petal Width.&lt;/p&gt;

&lt;h3 id=&quot;visualising-the-data-set&quot;&gt;Visualising the Data Set&lt;/h3&gt;
&lt;p&gt;Now let’s visualise the data set to get a better idea of what the dataset actually describes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Import dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

plt.figure(2, figsize=(8, 6))
plt.clf()

#Plot graph
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xticks(())
plt.yticks(())

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/irisgraph.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This graph shows the distribution of the three sample types, categorized by two of the features, sepal width and sepal length. Feel free to change the code and visualise the data in different ways.&lt;/p&gt;

&lt;h3 id=&quot;preparing-the-data&quot;&gt;Preparing the Data&lt;/h3&gt;
&lt;p&gt;We need to set aside some of the data set to become the data the perceptron is trained on. This data isn’t used again once the training is complete – the algorithm has already seen and learned from this information, and would serve no purpose to be classified – as the answer is already known.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Set aside 30% of the data set for training
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;StandardScaler()&lt;/code&gt; function standardised all of the features in a dataset to have a mean of zero and unit variance, to ensure all of the features are distributed normally. Many predictive algorithms require the data set to be standardised otherwise their behavior will be unpredictable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Train the scaler, which standarises all the features to have mean of zero and unit variance
sc = StandardScaler()
sc.fit(x_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we apply the scaler to each section of the split data set.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Apply the scaler to the X training data
x_train_std = sc.transform(x_train)

#Apply the SAME scaler to the X test data
x_test_std = sc.transform(x_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;scikit-learn-perceptron&quot;&gt;Scikit-Learn Perceptron&lt;/h3&gt;
&lt;p&gt;Here we use the &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron()&lt;/code&gt; function to invoke a perceptron object with the following parameters:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;max_iter&lt;/code&gt; is set to a maximum of 50 iterations over the data set, iterations are also referred to as epochs. Remember this word for if you plan to read further texts, it can be confusing having words used interchangeably.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;eta0&lt;/code&gt; is a constant value by which updates are multiplied. This will ensure the values change and don’t stagnate, it can be thought of as the bias.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;verbose&lt;/code&gt; is a boolean flag, when set to 1 it will print information for each epoch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Create a perceptron object with 50 iterations over the data set, and a learning rate of 0.3
ppn = Perceptron(max_iter=50, eta0=1, verbose=0)

#Train the perceptron
ppn.fit(x_train_std, y_train)

Perceptron(alpha=0.0001, class_weight=None, eta0=1, fit_intercept=True,
      max_iter=50, n_iter=None, n_jobs=1, penalty=None, random_state=0,
      shuffle=True, tol=None, verbose=0, warm_start=False)

#Apply the trained perceptron on the X data to make predicts for the y test data
y_pred = ppn.predict(x_test_std)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;the-results&quot;&gt;The Results&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;
#Print the predicted y test data
y_pred

array([1, 2, 2, 1, 1, 0, 2, 2, 1, 2, 2, 0, 2, 1, 1, 2, 1, 0, 0, 1, 2, 1,
       2, 2, 1, 1, 0, 2, 2, 2, 0, 1, 1, 0, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1,
       1])

#Print the true y test data
y_test

array([1, 2, 2, 1, 1, 0, 2, 1, 1, 2, 2, 0, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0,
       2, 2, 0, 1, 0, 2, 2, 2, 0, 1, 1, 0, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1,
       1])

#Print the accuracy of the implementation
print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))

Accuracy: 0.89
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This implementation of the most simple neural network achieves high accuracy. Throughout multiple runs it achieved between 0.89 and 1.00 accuracy, or said another way it’s 89% to 100% accurate.&lt;/p&gt;

&lt;p&gt;This was achieved following the recommended parameter settings found in the scikit-learn documentation. However, trial and error manipulation of the parameters can be an important method in machine learning, so don’t be afraid to experiment. Always consider how the parameters are being used internally, how they affect the data and experiment to see how changing them actually affects the result.&lt;/p&gt;

&lt;p&gt;Try changing the parameters for yourself to see how that affects the accuracy. See the scikit-learn perceptron documentation to see other parameters which you can manipulate. Hyperparameter manipulation is an important part of tuning and getting the best out of neural network implementations, as you read through the chapters of this book you will learn more about different parameters and continue to try your hand at practical exercises. Tuning is also discussed in detail in the Tuning and Evaluating Neural Networks chapter.&lt;/p&gt;

&lt;h3 id=&quot;under-the-hood-the-scikit-learn-perceptron&quot;&gt;Under the Hood: The Scikit-Learn Perceptron&lt;/h3&gt;
&lt;p&gt;Programming libraries are created to make the programmers life as easy as possible by providing commonly needed applications written by others. With this as their main aim libraries may not implement algorithms straight from textbooks. Additionally, as libraries are created with modularity in mind, their underlying implementation will likely be more complex than expected.&lt;/p&gt;

&lt;p&gt;Scikit-learn’s perceptron is no different. The &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron()&lt;/code&gt; function shares the underlying implementation of the SGDClassifier() function, and its functionality differs from traditional simple perceptrons as a result. For example, traditionally perceptrons required data to be linearly separable, that is to be able to be separated cleanly without overlap – imagine a graph with a single line separating two distinct groups of data points on each side.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SGDClassifier()&lt;/code&gt; implements regularized linear models utilizing the stochastic gradient descent (SGD) learning algorithm. &lt;code class=&quot;highlighter-rouge&quot;&gt;SGDClassifier(loss=“perceptron”, eta0=1, learning_rate=“constant”, penalty=None)&lt;/code&gt; is equivalent to &lt;code class=&quot;highlighter-rouge&quot;&gt;Perceptron()&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;jargon-buster&quot;&gt;Jargon Buster&lt;/h3&gt;
&lt;p&gt;That last bit became a little complicated, so lets break it down. “&lt;code class=&quot;highlighter-rouge&quot;&gt;SGDClassifier()&lt;/code&gt; implements regularized linear models utilizing the stochastic gradient descent (SGD) learning algorithm”&lt;/p&gt;

&lt;p&gt;Regularized by adding a penalty to the loss function that reduces the model parameters towards the zero vector, 0.0 on a graph, using one of two methods L1 or L2. Regularization increases the efficiency of the network. This will be discussed more in the chapter on Training Neural Networks.&lt;/p&gt;

&lt;p&gt;Linear model, referring to a type of classifier model which defines categories based on the linear combination of a features characteristics. That is a mathematical expression created by multiplying a set of characteristics by a constant and summing the result. &lt;code class=&quot;highlighter-rouge&quot;&gt;X1 w1 + b&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Stochastic Gradient Descent (SGD) is a type of backpropagation learning algorithm that calculates gradients based on the values of a batch with the aim of reducing the gradient (imagine a graph with a steadily decreasing learning rate gradient), hence gradient descent. The algorithm splits the data into batches and the gradients are summed and weights updated, similar to backpropagation.&lt;/p&gt;
</description>
        <pubDate>Wed, 22 Aug 2018 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/a-practical-introduction-to-artificial-neural-networks-with-python</link>
        <guid isPermaLink="true">http://localhost:4000/a-practical-introduction-to-artificial-neural-networks-with-python</guid>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>Why You Should Get into Machine Learning</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Four of the top ten and the two highest growth jobs are machine learning related according to LinkedIn.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;growth&quot;&gt;Growth&lt;/h3&gt;
&lt;p&gt;Since 2006 the machine learning field has been growing at an unprecedented pace. New developments each year push growth upwards and it shows no signs of slowing. Machine Learning has capabilities unheard of in computer science, from the outside it seems very close to human level intellect. Sadly, this is not the case.&lt;/p&gt;

&lt;p&gt;However, machine learning techniques and methods are some of the most powerful created and provided the new state-of-the-art in most of the areas they are applied. This has further increased the growth as academics, individuals and businesses hear of the abilities of these techniques they want to apply them to new problems.&lt;/p&gt;

&lt;p&gt;Google Trends can provide anecdotal evidence of the growth of machine learning. Below is a graph produced by Google Trends using Machine Learning as a search term plotting the interest globally over the last five years.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/interest-over-time-machine-learning.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Further anecdotal evidence of growth can be seen analysing specific machine learning tools and libraries using Google Trends, “TensorFlow”, “learn TensorFlow”, “Scikit learn”, etc.&lt;/p&gt;

&lt;h3 id=&quot;mergers-and-acquisitions&quot;&gt;Mergers and Acquisitions&lt;/h3&gt;
&lt;p&gt;The value of machine learning has not been lost on major corporations in the technology sector. Google, Facebook, Apple have acquired and absorbed business focusing on machine learning into their various areas of their business to add functionality to their products.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/aquisition-timeline.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The timeline above shows the acquisitions of Google, Apple and Facebook since 2010. At first glance you can see the increased speed of acquisitions. Upon closer inspection, you will notice acquisitions relating to existing services, or new services which launched soon after the acquisition. Mergers and acquisitions are often shrouded in secrecy to protect commercially sensitive information, so it can be difficult to gain much insight into the transactions, however researching the companies listed can provide some interesting information. Others are more obvious.&lt;/p&gt;

&lt;p&gt;Siri, acquired by Apple in 2010, became the core of Apple’s mobile virtual assistant. Google has made a number of acquisitions to further its own virtual assistant, Banter and API.ai provide conversational AI and advanced natural language processing.&lt;/p&gt;

&lt;h3 id=&quot;career&quot;&gt;Career&lt;/h3&gt;
&lt;p&gt;As businesses increase the size of their machine learning workforce; forming new machine learning projects and attempt to keep up with competitors, an opportunity presents itself for the reader. According to research conducted by LinkedIn, Machine Learning positions occupy the top spots for the fastest growing jobs on LinkedIn.&lt;/p&gt;

&lt;p&gt;LinkedIn monitored the growth of users adding their current position in various fields and compared the results over five years (2013-2017) and compiled an ‘Emerging Jobs Report 2017’. The following are machine learning specific.&lt;/p&gt;

&lt;p&gt;Machine Learning Engineer 9.8X growth.
Data Scientist 6.5X growth.
Big Data Developer 5.5X growth.
Director of Data Science 4.9X growth.
Four of the top ten, and the two highest growth jobs in the report are machine learning related. Incidentally, another three are within the computing field making computing easily the fastest growing job sector on LinkedIn.&lt;/p&gt;

&lt;p&gt;Earning potential should also be considered. Salaries will not be discussed here, as they are highly dependant on location. However, the job intelligence websites Payscale and Glassdoor can provide an indication, or detailed stats on the earning potential of various jobs.&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;

&lt;h4 id=&quot;hardware&quot;&gt;Hardware&lt;/h4&gt;

&lt;p&gt;One of the largest historical obstacles to machine learning progressing as a field was the limited availability and power of computing hardware. This is no longer the obstacle it used to be; the growth and advancement of computer hardware, specifically graphical processing units (GPUs), and the advent and growth of infrastructure as a service (IaaS) and platform as a service (PaaS).&lt;/p&gt;

&lt;p&gt;An unexpected accelerator of the computer hardware industry has been the cryptocurrency industry. In early 2018 enthusiastic crypto miners looking to make high profits created a global shortage of GPUs, causing some retailers to limit the number of cards individuals could purchase. During this GPU card shortage, industrial crypto miners started purchasing the processor wafer, the material CPUs/GPUs are made of, to create their own custom cards. In January 2017 the manufacturer TSMC were selling more 16nm wafer to crypto miners than nVidia.&lt;/p&gt;

&lt;p&gt;Ignoring what this means for the average consumer, I would argue the increased demand and revenue for card manufacturers and wafer manufacturers can only accelerate the creation of faster and more efficient hardware, which will become the backbone of future machine learning research and commercial endeavors.&lt;/p&gt;

&lt;h4 id=&quot;software&quot;&gt;Software&lt;/h4&gt;

&lt;p&gt;Powerful open source machine learning software libraries have emerged over the last five years. The most popular of these machine learning libraries have come from powerhouses of computing such as Google, Facebook and Microsoft. Written by experts in their field, tested and used on real-world problems and programs within their origin company. And they’re available for you to use.&lt;/p&gt;

&lt;p&gt;TensorFlow is one such library. The most popular machine learning library on GitHub, created by Google’s Brain team and open sourced in 2015. A unique visual dashboard allows the visualisation of complex machine learning algorithms, and the user base numbers in the thousands and a number of informative and tutorial books have been released, so support is readily available. For these reasons, this book uses TensorFlow for a number of the projects in future chapters.&lt;/p&gt;

&lt;h3 id=&quot;impact&quot;&gt;Impact&lt;/h3&gt;
&lt;p&gt;Machine Learning is already having a positive impact on economies and the lives of the product users.&lt;/p&gt;

&lt;p&gt;Seize the opportunity.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;Google Trends: machine Learning (2018). [Accesed July 2018]. https://trends.google.com/trends/explore?date=today%205-y&amp;amp;q=Machine%20Learning&lt;/p&gt;

&lt;p&gt;CB Insights: The Race for AI (2018). [Accessed July 2018]. https://www.cbinsights.com/research/top-acquirers-ai-startups-ma-timeline&lt;/p&gt;

&lt;p&gt;LinkedIn: The Fastest Growing Jobs in the US (2018). [Accessed July 2018]. https://blog.linkedin.com/2017/december/7/the-fastest-growing-jobs-in-the-u-s-based-on-linkedin-data&lt;/p&gt;

&lt;p&gt;LinkedIn: Emerging US Jobs Report 2017 (2018). [Accessed July 2018]. https://economicgraph.linkedin.com/research/LinkedIns-2017-US-Emerging-Jobs-Report&lt;/p&gt;

&lt;p&gt;PC GAMES N: TSMC are currently selling more 16nm chips to cryptocurrency miners than Nvidia (2018). [Accessed July 2018]. https://www.pcgamesn.com/tsmc-bitcoin-supply-nvidia&lt;/p&gt;

&lt;p&gt;Dark Vision Hardware: Bitmain is buying 20k 16nm wafers from TSMC per month (2018). [Accessed July 2018]. https://www.dvhardware.net/article68109.html&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Jul 2018 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/why-you-should-get-into-machine-learning</link>
        <guid isPermaLink="true">http://localhost:4000/why-you-should-get-into-machine-learning</guid>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>A Quick Introduction to Artificial Neural Networks (Part 2)</title>
        <description>&lt;h3 id=&quot;activation-functions&quot;&gt;Activation Functions&lt;/h3&gt;
&lt;p&gt;In &lt;a href=&quot;/a-quick-introduction-to-artificial-neural-networks-part-1&quot;&gt;part one&lt;/a&gt; of this Quick Introduction to Artificial Neural Networks we examined a diagram of a neural network. This part will discuss activation functions. Activation functions set a threshold which determines the frequency at which artificial neurons fire (their output). In biological neurons the precise timing of the neuron firing conveys information, in artificial neurons only the frequency is important.&lt;/p&gt;

&lt;h3 id=&quot;linear-activation-function&quot;&gt;Linear Activation Function&lt;/h3&gt;
&lt;p&gt;The linear activation function simply returns the neuron input value as its output value, without manipulating it at all. This type of activation function is often used where regression is to be applied, and the network is to learn to output numeric values. For example, modelling linear regression, a statistical technique for determining the relationship between values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/linear-activation-function.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-activation-function&quot;&gt;Step Activation Function&lt;/h3&gt;
&lt;p&gt;The step activation function only output two values based on input, 0 or 1. If the input value is below 0.5 the returned value is 0, if the input value is 0.5 or above the returned value is 1. This allows the mapping of boolean values, true and false.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/step-activation-function.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sigmoid-activation-function&quot;&gt;Sigmoid Activation Function&lt;/h3&gt;
&lt;p&gt;The Sigmoid activation function compresses input values to values between 0 and 1.0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/sigmoid-activation-function.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hyperbolic-tangent-activation-function&quot;&gt;Hyperbolic Tangent Activation Function&lt;/h3&gt;
&lt;p&gt;The Hyperbolic Tangent activation function, also called the tanh activation function conforms input signals to values between -1.0 and 1.0. It is similar to the Sigmoid activation function, with the additional value range of 0 to -1.0.&lt;/p&gt;

&lt;p&gt;This additional range proves useful when the input values to the network are negative. In such circumstances the Sigmoid function reduces such negative values to near 0, which results in the networks parameters being updated less regularly and negatively affecting training. The Tanh function does not suffer this same flaw, as negative values are not forced into a positive range.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/hyperbolic-tangent-activation-function.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rectified-linear-units-relu&quot;&gt;Rectified Linear Units (ReLU)&lt;/h3&gt;
&lt;p&gt;The Rectified Linear Units (ReLU) activation function does not compress values to a small range like the Sigmoid or Tanh activation functions, which compress input values to 0 to 1, or -1 to 1.0 respectively. The increased range offered by ReLU results in far superior performance in training compared to both Sigmoid and Tanh.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/ReLU-activation-function.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;softmax-activation-function&quot;&gt;Softmax Activation Function&lt;/h3&gt;
&lt;p&gt;The Softmax activation function compresses values to positive values between 0.0 and 1.0. Typically placed in output layers of networks used for classification, the Softmax neurons allow the prediction of outputs to certain classes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/softmax-activation-function.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Summary
It can be difficult to grasp the usefulness of the different activation functions, at first glance they all look the very similar. Further, how do you decide between them? Luckily, there are best practices and established use-cases. For example, the Softmax activation function is usually found in the output layer of a classification network, the Softmax neuron with the highest value dictates the predicted class of the input value – remember that the neurons have been trained and thus it can be said that the neurons belong to certain classes as decided by the training.&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jun 2018 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/a-quick-introduction-to-artificial-neural-networks-part-2</link>
        <guid isPermaLink="true">http://localhost:4000/a-quick-introduction-to-artificial-neural-networks-part-2</guid>
        
        <category>Machine Learning</category>
        
        <category>Neural Networks</category>
        
        
      </item>
    
      <item>
        <title>A Quick Introduction to Artificial Neural Networks (Part 1)</title>
        <description>&lt;h3 id=&quot;artificial-neural-networks&quot;&gt;Artificial Neural Networks&lt;/h3&gt;
&lt;p&gt;Artificial neurons mimic the basic function of biological neurons, and much like their biological counterparts they only become useful when connected in a larger network, called Artificial Neural Networks.&lt;/p&gt;

&lt;p&gt;Neural networks are extremely adept at recognising patterns, this is pattern recognition is the reason they have become so widely used and talked about in recent years. The ability of neural networks to discover and recognise patterns allows them to far surpass traditional methods in computer science, making them viable for use in research and business.&lt;/p&gt;

&lt;h3 id=&quot;an-artificial-neuron&quot;&gt;An Artificial Neuron&lt;/h3&gt;
&lt;p&gt;The image below show an illustration of a single biological neuron annotated to describe a single artificial neurons function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/neuron-annotated.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A biological neuron receives input signals from its dendrites from other neurons and sends output signals along its axon, which branches out and connects to other neurons. In the illustration above, the input signal is represented by &lt;code class=&quot;highlighter-rouge&quot;&gt;x0&lt;/code&gt;, as this signal ‘travels’ it is multiplied (&lt;code class=&quot;highlighter-rouge&quot;&gt;w0 x0&lt;/code&gt;) based on the a weight variable (&lt;code class=&quot;highlighter-rouge&quot;&gt;w0&lt;/code&gt;). The weight variables are learnable and the weights strength and polarity (positive or negative) control the influence of the signal. The influence is determined by summing the signal input and weight (&lt;code class=&quot;highlighter-rouge&quot;&gt;∑i wi xi + b&lt;/code&gt;) which is then calculated by the activation function f, if it is above a certain threshold the neuron fires.&lt;/p&gt;

&lt;h3 id=&quot;activation-functions-in-brief&quot;&gt;Activation Functions in Brief&lt;/h3&gt;
&lt;p&gt;The frequency of the firing of an artificial neuron is determined by an activation function. It determines at what threshold the neuron will fire. This section describes the Sigmoid activation function to bring clarity to the operation of an artificial neuron. Activation functions will be discussed in detail further on in this guide.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/posts/2018/sigmoidexample.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph above shows the Sigmoid activation function, which will convert all input signals into positive values between 0 and 1. The use of other activation functions with wider values, or negative values change the frequency of the firing of a neuron and will suit different types of data and purposes.&lt;/p&gt;

&lt;p&gt;Activation functions are dicussed in further detail in the final quick introduction post, &lt;a href=&quot;/a-quick-introduction-to-artificial-neural-networks-part-2&quot;&gt;part two&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Apr 2018 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/a-quick-introduction-to-artificial-neural-networks-part-1</link>
        <guid isPermaLink="true">http://localhost:4000/a-quick-introduction-to-artificial-neural-networks-part-1</guid>
        
        <category>Machine Learning</category>
        
        <category>Neural Networks</category>
        
        
      </item>
    
      <item>
        <title>Could Sentiment Analysis and Opinion Mining Improve Your Project?</title>
        <description>&lt;h3 id=&quot;what-is-sentiment-analysis&quot;&gt;What is Sentiment Analysis?&lt;/h3&gt;
&lt;p&gt;Sentiment Analysis and Opinion Mining (SAOM) is an area of machine learning in computer science which attempts to derive sentiment – a users opinion or feeling – from text. Sentiment analysis has become a hot topic over recent years in academia and in the business world, as its value has become clear. Researchers have created and examined powerful new methods to achieve near human, or in some cases, surpass human ability to recognise sentiment, and far surpass the human ability to analyse the shear quantity of data available. Businesses have recognised it as a tool for customer engagement, project success/failure measurement, creation of brand advocates, brand monitoring and much more.&lt;/p&gt;

&lt;h3 id=&quot;why-would-you-measure-opinion&quot;&gt;Why would you measure opinion?&lt;/h3&gt;
&lt;p&gt;Measuring opinion is about creating insight. Insight into customer/user option which can be used to determine the direction of a project, measure how a project has been received, even monitor opinion on an issue in society or political party, and so much more. Computational methods of measuring opinion have many benefits over traditional methods, two of the most important and most valuable allow the analysis process to be automated and constant.&lt;/p&gt;

&lt;p&gt;Successfully encouraging users to take an action is hard. Conversion rates are typically in single digits for most actions online. Attempting to convert users to freely offer their time to fill out a form – where the results may be statistically insignificant due to a small sample size is hard and quite often fruitless. Machine learning and sentiment analysis allows the option to gather the type of data users would fill in on a form, passively, without interacting with the user, via the information they freely share online on social networking sites, blogs, forums, and the rest.&lt;/p&gt;

&lt;p&gt;The fact this can be done automatically, and in the case of social networks which are in most cases “real-time”, where users share messages while they take actions, allowing near live data. This can be immensely valuable depending on the use-case.&lt;/p&gt;

&lt;p&gt;This isn’t to say sentiment analysis should replace focus groups or surveys entirely, these have their place. Sentiment analysis is just another tool in your toolbox.&lt;/p&gt;

&lt;h3 id=&quot;sentiment-analysis-in-practise&quot;&gt;Sentiment Analysis in Practise&lt;/h3&gt;
&lt;p&gt;One of my lecturers at university, Dr. Kevan Buckley,  was involved in a project called SentiStrength. SentiStrength measures the sentiment of tweets on Twitter, either positive or negative. It was used during the 2012 Olympics in London to display the visual mood of the nation from twitter towards the games in colourful lights on the London Eye, the large ferris wheel permanently stationed on the banks of the River Thames. SentiStrength was also put to use during the the UK Summer Riots of 2011, analysing 2.6 million tweets in an attempt to understand the disorder. Gathering data during these large events without automated, remote and constant analysis would be extremely costly. Hundreds of surveyors would need to be deployed and briefed to gather an appropriate sample size and cross section of the attendees of these events. Not to mention most rioters don’t want to stick around and fill in surveys.&lt;/p&gt;

&lt;h3 id=&quot;is-sentiment-analysis-right-for-your-project&quot;&gt;Is Sentiment Analysis Right for Your Project?&lt;/h3&gt;
&lt;p&gt;That is down to you. I’m an advocate of using computational methods to analyse opinion due to the advantages described above, I hope this short article has achieved its objective and made you consider measuring opinion computationally.&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Mar 2018 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/could-sentiment-analysis-and-opinion-mining-improve-your-project</link>
        <guid isPermaLink="true">http://localhost:4000/could-sentiment-analysis-and-opinion-mining-improve-your-project</guid>
        
        <category>Sentiment Analysis</category>
        
        <category>Machine Learning</category>
        
        
      </item>
    
      <item>
        <title>The C Programming Language (Book Review)</title>
        <description>&lt;p&gt;&lt;em&gt;Brian W. Kernighan, Dennis M. Ritchie. &lt;a href=&quot;https://amzn.to/2RHfTKD&quot;&gt;The C Programming Language. 2nd Edition&lt;/a&gt;. Upper Saddle River, NJ, USA: Prentice-Hall, Inc. 2014. ISBN 0-13-110362-8, 0-13-110370-9. GBP£49 / USD$63&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“This book was typeset in Times New Roman and Courier by the authors, using an Autologic APS-5 phototypesetter and a DEC VAX 8550 running the 9th Edition of the UNIX operating system.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the proclamation on the copyright page of ‘The C Programming Book’. It crosses my mind I am writing this review on a Mac running OS X, a distant child of the 9th Edition of UNIX thirty-seven years later.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In its fifty-second printing the book is a classic computer science text. It is so revered among computer scientists because the original creator of C, Dennis Ritchie, is one of the authors. ‘The C Programming Language’ aims to teach beginner’s the C programming language from scratch via a quick “Tutorial introduction” and a series of example programs exploring various facets of the language in increasing difficulty.&lt;/p&gt;

&lt;p&gt;The second edition, and the most recent, was first printed in 1989 to include the various changes and additions after the American National Standards Institute (ANSI) and the International Organisation for Standardisation (ISO) created the C89/C90 standards, which for standardised the language for the first time. Despite being in its fifty-seventh printing, it hasn’t been updated in twenty-seven years, and as a result it is starting to show its age – if only in it use of language.&lt;/p&gt;

&lt;p&gt;‘The C Programming Language’ is noticeably ‘stuffy’ and verbose. At times it approaches a technical manual to my modern eyes. However, if this is the only major weakness, it can be overlooked due to the books status and relevant content. Yet, despite my nostalgia and reverence for ‘The C Programming Language’ I would argue there are much better books to learn C.&lt;/p&gt;

&lt;h3 id=&quot;comparison&quot;&gt;Comparison&lt;/h3&gt;
&lt;p&gt;When compared to modern texts such as ‘Learning C the Hard Way’, ‘The C Programming Language’ immediately looks wrinkled. This comparison is particularly apt as they each teach C in a similar way – through practical, increasingly difficult programming examples. In ‘Learn C the Hard Way’ succinct, lively, but professionally written chapters replace the verbose, stuffy language of ‘The C Programming Language’. In ’Learn C the Hard Way’ each chapter sets out a programming task centred around a language feature, each task builds a real-world program. While the authors of ‘The C Programming Language’, state “we have tried where possible to show useful programming techniques”, and “when forced to make a decision [on what to include], we have concentrated on the language”. This is not unreasonable, yet, ‘Learn C the Hard Way’ manages to create real-world “complete programs” and show off features of the language at the same time.&lt;/p&gt;

&lt;p&gt;Regardless of the criticism of the language of ‘The C Programming Language’ is unsurpassed as a technical reference. And despite its stuffiness and verbosity it does a good job of guiding the reader through aspects of the language. The authors concentration on the language is clear, and the result is a perfectly usable, but inferior to modern texts, introduction to C.&lt;/p&gt;

&lt;h3 id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;
&lt;p&gt;The book starts with ‘Chapter 1: Tutorial Introduction’, a 34 page, fast-paced, practical introduction to the language which encourages readers to write “complete programs” using the major features of C. This format is useful to the reader because they have gained some practical knowledge, so when they read the detailed sections which deal with those features they are more likely to understand the explanation. The book assumes the reader already has some basic programming knowledge, of compilation and algebraic expressions.&lt;/p&gt;

&lt;p&gt;Chapters 2 through 6 cover those features, and others, in greater detail. The chapters keep the focus on writing “complete programs”, as opposed to code snippets, which arguably leads to greater usefulness and comprehension for the reader. Chapter 7 describes the standard library, Chapter 8 deals with the interface between C programs and the UNIX operating system, and the final section, Appendix A, contains a reference manual.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h3&gt;
&lt;p&gt;’The C Programming Language’ is still a useful book to learn the C language, however let us not be held back by nostalgia and unmoving reverence. The book holds up surprisingly well for a twenty-seven-year-old revision of a thirty-seven-year-old book (its original printing). I argue if you are to buy only one book to learn C, don’t spend £33 on ‘The C Programming Language’. Consider ‘Learn C the Hard Way’, or another modern text. Have a look at some of the free PDF books which have gained popularity such as ‘Modern C’ from Prof. Jens Gustedt, a French academic and ISO expert in C.&lt;/p&gt;

&lt;p&gt;‘The C Programming Language’ can have a place on your bookshelf, but only among other C programming books.&lt;/p&gt;
</description>
        <pubDate>Sat, 17 Mar 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/the-c-programming-language-book-review</link>
        <guid isPermaLink="true">http://localhost:4000/the-c-programming-language-book-review</guid>
        
        <category>Books</category>
        
        <category>Reviews</category>
        
        
      </item>
    
  </channel>
</rss>
